{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meiaalsup/lda_infographics/blob/master/6_804.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7C46aXQ8mXq"
   },
   "source": [
    "6.804 Final Project\n",
    "\n",
    "Meia Alsup, Sravya Bhamidipati, Anelise Newman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heVQnEhe8jME"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcUwnbva74ov"
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML \n",
    "import urllib.request as request\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('raw_ocr_output.pickle','rb')\n",
    "#ocr = pkl.load(f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#less_ocr = {key: ocr[key] for i, key in enumerate(ocr) if i < 100}\n",
    "#less_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('icon_annotations_testing.pickle','rb')\n",
    "#icons = pkl.load(f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations from task where participants marked bounding boxes around all icons on an infographic\n",
    "icon_dict = pkl.load(open( \"icon_annotations_all.pickle\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(icon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "icons_english  = set(icon_dict.keys())\n",
    "#icons_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWx81Nf-8gV_"
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOPDW6QT8dTe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have extracted words from 62361 infographics.\n"
     ]
    }
   ],
   "source": [
    "f = open('google_text_extraction_output.pckl','rb')\n",
    "infographic_name_to_words = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "print('We have extracted words from %d infographics.'%len(infographic_name_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_english(set_of_words):\n",
    "    english = set(['the', 'she', 'different', 'time', 'and', 'client', 'sports', 'grammar', 'family', 'report'])\n",
    "    spanish = set(['el', 'es', 'cuando', 'esta', 'malo', 'que', 'tiene', 'para'])\n",
    "    if bool(set(set_of_words) & spanish):\n",
    "        return False\n",
    "    if bool(set(set_of_words) & english):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46952"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english  = {key: val for key, val in infographic_name_to_words.items() if contains_english(val)}\n",
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dump(english, open('filtered.json', 'w'))\n",
    "#json.dump([key for key, val in english.items()], open('filtered_keys.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [list_words for list_words in english.values()]\n",
    "newList = []\n",
    "stemmer = PorterStemmer()\n",
    "for wordList in words:\n",
    "    thisList = []\n",
    "    for word in wordList:\n",
    "        if len(word) > 2 and word.isalpha() and word not in STOPWORDS and wordnet.synsets(word):\n",
    "            thisList.append(word.lower())\n",
    "            #thisList.append(stemmer.stem(WordNetLemmatizer().lemmatize(word.lower(), pos='v')))\n",
    "    newList.append(thisList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46952"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(newList)\n",
    "common_dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
    "\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in newList]\n",
    "# Train the model on the corpus.\n",
    "id2token = {val:key for key, val in common_dictionary.token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0\n",
      "Top 20 terms: dict_keys(['skin', 'yes', 'cancer', 'are', 'breast', 'crime', 'cases', 'people', 'deaths', 'death', 'can', 'year', 'smoking', 'tobacco', 'radiation', 'smoke', 'not', 'criminal', 'common', 'years'])\n",
      "Topic number 1\n",
      "Top 20 terms: dict_keys(['job', 'work', 'employees', 'jobs', 'security', 'employee', 'workers', 'training', 'company', 'management', 'working', 'office', 'professional', 'companies', 'career', 'skills', 'salary', 'services', 'manager', 'employers'])\n",
      "Topic number 2\n",
      "Top 20 terms: dict_keys(['water', 'home', 'real', 'property', 'estate', 'solar', 'homes', 'house', 'average', 'price', 'gallons', 'rent', 'bed', 'housing', 'cost', 'properties', 'year', 'bugs', 'prices', 'buyers'])\n",
      "Topic number 3\n",
      "Top 20 terms: dict_keys(['social', 'media', 'business', 'twitter', 'marketing', 'businesses', 'small', 'use', 'brand', 'customers', 'local', 'online', 'post', 'users', 'companies', 'followers', 'posts', 'service', 'people', 'customer'])\n",
      "Topic number 4\n",
      "Top 20 terms: dict_keys(['risk', 'body', 'heart', 'blood', 'people', 'brain', 'sleep', 'disease', 'health', 'stress', 'weight', 'help', 'cause', 'pain', 'high', 'children', 'loss', 'exercise', 'healthy', 'use'])\n",
      "Topic number 5\n",
      "Top 20 terms: dict_keys(['feet', 'hotel', 'long', 'size', 'head', 'black', 'shoes', 'species', 'area', 'foot', 'inches', 'high', 'large', 'steel', 'beach', 'hair', 'length', 'white', 'hotels', 'flat'])\n",
      "Topic number 6\n",
      "Top 20 terms: dict_keys(['people', 'like', 'want', 'know', 'wedding', 'time', 'think', 'friends', 'ask', 'good', 'new', 'way', 'have', 'look', 'but', 'date', 'are', 'photos', 'love', 'send'])\n",
      "Topic number 7\n",
      "Top 20 terms: dict_keys(['data', 'business', 'customer', 'time', 'new', 'sales', 'management', 'technology', 'marketing', 'information', 'big', 'companies', 'need', 'strategy', 'performance', 'process', 'increase', 'lead', 'value', 'research'])\n",
      "Topic number 8\n",
      "Top 20 terms: dict_keys(['united', 'world', 'countries', 'states', 'south', 'population', 'average', 'global', 'north', 'million', 'number', 'total', 'china', 'country', 'international', 'america', 'new', 'australia', 'europe', 'india'])\n",
      "Topic number 9\n",
      "Top 20 terms: dict_keys(['home', 'new', 'space', 'room', 'house', 'kitchen', 'door', 'state', 'texas', 'bathroom', 'florida', 'california', 'furniture', 'light', 'moving', 'window', 'floor', 'carolina', 'design', 'paint'])\n",
      "Topic number 10\n",
      "Top 20 terms: dict_keys(['time', 'color', 'love', 'day', 'red', 'blue', 'best', 'white', 'round', 'art', 'colors', 'ring', 'popular', 'like', 'black', 'great', 'old', 'diamond', 'flowers', 'life'])\n",
      "Topic number 11\n",
      "Top 20 terms: dict_keys(['mobile', 'online', 'phone', 'users', 'use', 'devices', 'consumers', 'email', 'marketing', 'digital', 'internet', 'device', 'sales', 'apple', 'shopping', 'android', 'customers', 'store', 'phones', 'tablet'])\n",
      "Topic number 12\n",
      "Top 20 terms: dict_keys(['gold', 'team', 'sports', 'won', 'win', 'season', 'games', 'players', 'world', 'cup', 'league', 'football', 'player', 'record', 'series', 'final', 'game', 'wins', 'olympic', 'teams'])\n",
      "Topic number 13\n",
      "Top 20 terms: dict_keys(['energy', 'air', 'use', 'power', 'water', 'heat', 'gas', 'carbon', 'waste', 'save', 'electricity', 'green', 'heating', 'reduce', 'emissions', 'tons', 'year', 'plants', 'windows', 'efficiency'])\n",
      "Topic number 14\n",
      "Top 20 terms: dict_keys(['christmas', 'miles', 'park', 'west', 'years', 'royal', 'london', 'space', 'earth', 'street', 'north', 'world', 'john', 'history', 'george', 'king', 'british', 'war', 'prince', 'new'])\n",
      "Topic number 15\n",
      "Top 20 terms: dict_keys(['pet', 'dog', 'military', 'pets', 'dogs', 'ukraine', 'attacks', 'airlines', 'july', 'flight', 'location', 'guard', 'control', 'june', 'owners', 'forces', 'national', 'service', 'cat', 'coast'])\n",
      "Topic number 16\n",
      "Top 20 terms: dict_keys(['women', 'age', 'men', 'years', 'tax', 'average', 'year', 'people', 'income', 'percent', 'americans', 'children', 'state', 'million', 'are', 'more', 'american', 'likely', 'states', 'child'])\n",
      "Topic number 17\n",
      "Top 20 terms: dict_keys(['students', 'school', 'college', 'education', 'university', 'student', 'degree', 'high', 'schools', 'online', 'average', 'higher', 'debt', 'public', 'class', 'learning', 'science', 'state', 'teachers', 'year'])\n",
      "Topic number 18\n",
      "Top 20 terms: dict_keys(['food', 'eat', 'coffee', 'foods', 'calories', 'tea', 'fat', 'vitamin', 'protein', 'sugar', 'cup', 'eating', 'fruit', 'milk', 'cheese', 'healthy', 'drink', 'chocolate', 'fish', 'sweet'])\n",
      "Topic number 19\n",
      "Top 20 terms: dict_keys(['million', 'billion', 'day', 'average', 'year', 'users', 'hours', 'online', 'internet', 'number', 'total', 'travel', 'video', 'time', 'top', 'holiday', 'spent', 'month', 'spend', 'week'])\n",
      "Topic number 20\n",
      "Top 20 terms: dict_keys(['content', 'google', 'search', 'web', 'design', 'website', 'page', 'site', 'use', 'create', 'links', 'user', 'link', 'product', 'sites', 'blog', 'pages', 'information', 'quality', 'websites'])\n",
      "Topic number 21\n",
      "Top 20 terms: dict_keys(['growth', 'industry', 'billion', 'market', 'revenue', 'trade', 'investment', 'production', 'total', 'oil', 'companies', 'services', 'economic', 'value', 'global', 'capital', 'sector', 'economy', 'stock', 'demand'])\n",
      "Topic number 22\n",
      "Top 20 terms: dict_keys(['game', 'games', 'super', 'wine', 'play', 'beer', 'bowl', 'movie', 'kids', 'movies', 'drink', 'playing', 'poker', 'fun', 'glass', 'best', 'favorite', 'like', 'fashion', 'gun'])\n",
      "Topic number 23\n",
      "Top 20 terms: dict_keys(['credit', 'money', 'cloud', 'cost', 'card', 'pay', 'business', 'financial', 'costs', 'debt', 'cards', 'save', 'service', 'bank', 'savings', 'time', 'loan', 'payment', 'cash', 'services'])\n",
      "Topic number 24\n",
      "Top 20 terms: dict_keys(['use', 'sure', 'need', 'make', 'keep', 'tips', 'check', 'avoid', 'look', 'time', 'right', 'good', 'try', 'help', 'get', 'step', 'clean', 'away', 'place', 'like'])\n",
      "Topic number 25\n",
      "Top 20 terms: dict_keys(['people', 'new', 'community', 'work', 'help', 'members', 'team', 'music', 'learn', 'support', 'learning', 'skills', 'are', 'group', 'create', 'vote', 'important', 'world', 'events', 'language'])\n",
      "Topic number 26\n",
      "Top 20 terms: dict_keys(['insurance', 'injuries', 'camera', 'accidents', 'injury', 'theft', 'claims', 'police', 'stolen', 'accident', 'fatal', 'case', 'damage', 'law', 'personal', 'mph', 'life', 'flash', 'claim', 'lost'])\n",
      "Topic number 27\n",
      "Top 20 terms: dict_keys(['car', 'driving', 'cars', 'vehicle', 'drivers', 'road', 'drive', 'fuel', 'driver', 'engine', 'safety', 'motor', 'vehicles', 'speed', 'truck', 'auto', 'traffic', 'ford', 'seat', 'miles'])\n",
      "Topic number 28\n",
      "Top 20 terms: dict_keys(['new', 'city', 'york', 'first', 'book', 'jan', 'printing', 'square', 'meters', 'led', 'chicago', 'cost', 'world', 'built', 'mar', 'may', 'center', 'building', 'cities', 'print'])\n",
      "Topic number 29\n",
      "Top 20 terms: dict_keys(['health', 'care', 'medical', 'treatment', 'patients', 'hair', 'dental', 'healthcare', 'teeth', 'surgery', 'patient', 'injuries', 'hospital', 'doctor', 'provided', 'tooth', 'people', 'procedures', 'million', 'nursing'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_TOPICS):\n",
    "    terms = lda.get_topic_terms(i, 20)\n",
    "    print(f'Topic number {i}')\n",
    "    val = {id2token[t[0]]: t[1] for t in terms}\n",
    "    #print(f'Top 20 terms: {val}')\n",
    "    print(f'Top 20 terms: {val.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = lda.save('saved_lda_30_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['free-autism-infographic_564c4ed54ea8f.jpg',\n",
       " 'lemonlys-one-sweet-year-2012-annual-report_51141ef25ba83.jpg',\n",
       " 'top-10-darkest-characters-in-film_544fde05817b7.jpg',\n",
       " '12-christmas-traditions-from-around-the-world_50290c852911e.jpg',\n",
       " 'dc-power-on-board-lowering-environmental-impact_5029160e8b05c.png',\n",
       " 'common-health-issues-veterans-face_5385e1a9af293.jpg',\n",
       " '6-classic-coat-styles-for-men_5306499534889.jpg',\n",
       " 'interesting-facts-about-teeth_53d291913ac5f.jpg',\n",
       " 'databagg--online-storage-service_53cf95209c581.jpg',\n",
       " 'diabetes-month-an-infographic_52791b9fc14d8.jpg']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(english.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='http://thumbnails.visually.netdna-cdn.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0824c416f255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'http://thumbnails.visually.netdna-cdn.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0murl_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#display(Image(url=f'{base}/{url_}'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base}/{url_}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'images/{url_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'english' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for url_ in list(english.keys())[30:150]:\n",
    "    #display(Image(url=f'{base}/{url_}'))\n",
    "    request.urlretrieve(f'{base}/{url_}', f'images/{url_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set([url_ for url_ in english.keys() if url_ in icons_english])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url_ in intersection:\n",
    "    #display(Image(url=f'{base}/{url_}'))\n",
    "    request.urlretrieve(f'{base}/{url_}', f'images/{url_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "6.804.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
