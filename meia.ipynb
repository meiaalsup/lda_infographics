{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7C46aXQ8mXq"
   },
   "source": [
    "# 6.804 Final Project\n",
    "\n",
    "Meia Alsup, Sravya Bhamidipati, Anelise Newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcUwnbva74ov"
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML \n",
    "import urllib.request as request\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ocr = pkl.load(open('raw_ocr_output.pickle','rb'))\n",
    "#less_ocr = {key: ocr[key] for i, key in enumerate(ocr) if i < 100}\n",
    "#less_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('icon_annotations_testing.pickle','rb')\n",
    "#icons = pkl.load(f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean input to just English words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOPDW6QT8dTe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have extracted words from 62361 infographics.\n"
     ]
    }
   ],
   "source": [
    "f = open('google_text_extraction_output.pckl','rb')\n",
    "infographic_name_to_words = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "print('We have extracted words from %d infographics.'%len(infographic_name_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_english(set_of_words):\n",
    "    english = set(['the', 'she', 'different', 'time', 'and', 'client', 'sports', 'grammar', 'family', 'report'])\n",
    "    spanish = set(['el', 'es', 'cuando', 'esta', 'malo', 'que', 'tiene', 'para'])\n",
    "    if bool(set(set_of_words) & spanish):\n",
    "        return False\n",
    "    if bool(set(set_of_words) & english):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46952"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english  = {key: val for key, val in infographic_name_to_words.items() if contains_english(val)}\n",
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(english, open('filtered.json', 'w'))\n",
    "json.dump([key for key, val in english.items()], open('filtered_keys.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [list_words for list_words in english.values()]\n",
    "newList = []\n",
    "stemmer = PorterStemmer()\n",
    "for wordList in words:\n",
    "    thisList = []\n",
    "    for word in wordList:\n",
    "        if len(word) > 2 and word.isalpha() and word not in STOPWORDS and wordnet.synsets(word):\n",
    "            thisList.append(word.lower())\n",
    "            #thisList.append(stemmer.stem(WordNetLemmatizer().lemmatize(word.lower(), pos='v')))\n",
    "    newList.append(thisList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWx81Nf-8gV_"
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(newList)\n",
    "common_dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
    "\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in newList]\n",
    "# Train the model on the corpus.\n",
    "id2token = {val:key for key, val in common_dictionary.token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0\n",
      "Top 20 terms: dict_keys(['home', 'price', 'market', 'real', 'average', 'property', 'value', 'estate', 'money', 'sales', 'house', 'prices', 'buy', 'new', 'stock', 'buying', 'sell', 'london', 'homes', 'investment'])\n",
      "Topic number 1\n",
      "Top 20 terms: dict_keys(['social', 'media', 'marketing', 'content', 'customer', 'customers', 'people', 'email', 'use', 'twitter', 'brand', 'business', 'post', 'marketers', 'service', 'page', 'blog', 'share', 'create', 'like'])\n",
      "Topic number 2\n",
      "Top 20 terms: dict_keys(['million', 'united', 'states', 'world', 'new', 'years', 'number', 'south', 'population', 'state', 'countries', 'north', 'year', 'people', 'national', 'total', 'average', 'america', 'american', 'age'])\n",
      "Topic number 3\n",
      "Top 20 terms: dict_keys(['users', 'online', 'internet', 'google', 'mobile', 'search', 'web', 'million', 'video', 'website', 'site', 'billion', 'social', 'use', 'user', 'sites', 'digital', 'content', 'data', 'people'])\n",
      "Topic number 4\n",
      "Top 20 terms: dict_keys(['energy', 'water', 'power', 'gas', 'air', 'solar', 'use', 'oil', 'home', 'carbon', 'electricity', 'light', 'heat', 'save', 'fuel', 'heating', 'green', 'emissions', 'natural', 'year'])\n",
      "Topic number 5\n",
      "Top 20 terms: dict_keys(['car', 'cars', 'home', 'vehicle', 'air', 'miles', 'engine', 'vehicles', 'kitchen', 'new', 'auto', 'room', 'fuel', 'space', 'cost', 'damage', 'bed', 'bathroom', 'drive', 'motor'])\n",
      "Topic number 6\n",
      "Top 20 terms: dict_keys(['top', 'job', 'team', 'new', 'march', 'president', 'news', 'best', 'vote', 'social', 'most', 'jobs', 'twitter', 'design', 'people', 'score', 'media', 'february', 'political', 'members'])\n",
      "Topic number 7\n",
      "Top 20 terms: dict_keys(['credit', 'debt', 'card', 'dog', 'loan', 'pet', 'water', 'cards', 'loans', 'dogs', 'average', 'military', 'ukraine', 'pets', 'cat', 'bankruptcy', 'control', 'food', 'attacks', 'score'])\n",
      "Topic number 8\n",
      "Top 20 terms: dict_keys(['color', 'design', 'look', 'style', 'colors', 'like', 'white', 'different', 'blue', 'size', 'black', 'light', 'wear', 'right', 'best', 'red', 'shoes', 'perfect', 'use', 'shape'])\n",
      "Topic number 9\n",
      "Top 20 terms: dict_keys(['game', 'games', 'first', 'hair', 'music', 'players', 'super', 'play', 'time', 'player', 'million', 'team', 'bowl', 'round', 'all', 'cup', 'season', 'printing', 'ball', 'record'])\n",
      "Topic number 10\n",
      "Top 20 terms: dict_keys(['mobile', 'phone', 'shopping', 'apple', 'devices', 'use', 'online', 'android', 'store', 'device', 'phones', 'holiday', 'consumers', 'tablet', 'sales', 'retail', 'purchase', 'shoppers', 'cell', 'free'])\n",
      "Topic number 11\n",
      "Top 20 terms: dict_keys(['work', 'students', 'school', 'college', 'education', 'time', 'job', 'people', 'help', 'employees', 'university', 'are', 'working', 'learning', 'need', 'degree', 'high', 'workers', 'student', 'skills'])\n",
      "Topic number 12\n",
      "Top 20 terms: dict_keys(['food', 'water', 'coffee', 'wine', 'calories', 'eat', 'foods', 'tea', 'drink', 'fish', 'beer', 'green', 'gold', 'cup', 'fat', 'day', 'ice', 'red', 'hot', 'protein'])\n",
      "Topic number 13\n",
      "Top 20 terms: dict_keys(['security', 'data', 'information', 'use', 'drug', 'safety', 'child', 'drugs', 'protect', 'theft', 'abuse', 'children', 'access', 'protection', 'personal', 'identity', 'stolen', 'fraud', 'crime', 'safe'])\n",
      "Topic number 14\n",
      "Top 20 terms: dict_keys(['use', 'need', 'sure', 'time', 'make', 'step', 'help', 'right', 'easy', 'process', 'good', 'avoid', 'tips', 'keep', 'check', 'test', 'ensure', 'free', 'know', 'like'])\n",
      "Topic number 15\n",
      "Top 20 terms: dict_keys(['yes', 'driving', 'injuries', 'drivers', 'injury', 'deaths', 'accidents', 'road', 'death', 'are', 'people', 'year', 'traffic', 'driver', 'accident', 'safety', 'fatal', 'crashes', 'speed', 'car'])\n",
      "Topic number 16\n",
      "Top 20 terms: dict_keys(['billion', 'cost', 'insurance', 'million', 'average', 'tax', 'health', 'costs', 'care', 'pay', 'total', 'income', 'financial', 'year', 'annual', 'money', 'services', 'medical', 'spending', 'federal'])\n",
      "Topic number 17\n",
      "Top 20 terms: dict_keys(['women', 'health', 'men', 'people', 'heart', 'skin', 'blood', 'risk', 'disease', 'body', 'sleep', 'cancer', 'treatment', 'brain', 'help', 'pain', 'age', 'healthy', 'stress', 'cause'])\n",
      "Topic number 18\n",
      "Top 20 terms: dict_keys(['business', 'data', 'companies', 'cloud', 'growth', 'new', 'technology', 'management', 'industry', 'businesses', 'global', 'market', 'sales', 'services', 'development', 'small', 'company', 'increase', 'software', 'support'])\n",
      "Topic number 19\n",
      "Top 20 terms: dict_keys(['day', 'travel', 'new', 'time', 'love', 'people', 'wedding', 'like', 'best', 'family', 'world', 'great', 'book', 'hotel', 'year', 'big', 'man', 'christmas', 'fun', 'know'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_TOPICS):\n",
    "    terms = lda.get_topic_terms(i, 20)\n",
    "    print(f'Topic number {i}')\n",
    "    val = {id2token[t[0]]: t[1] for t in terms}\n",
    "    #print(f'Top 20 terms: {val}')\n",
    "    print(f'Top 20 terms: {val.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = lda.save('saved_lda_15_topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersction with infographics that have labeled bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations from task where participants marked bounding boxes around all icons on an infographic\n",
    "icon_dict = pkl.load(open( \"icon_annotations_all.pickle\", \"rb\") )\n",
    "icons_english  = set(icon_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='http://thumbnails.visually.netdna-cdn.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set([url_ for url_ in english.keys() if url_ in icons_english])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url_ in intersection:\n",
    "    #display(Image(url=f'{base}/{url_}'))\n",
    "    request.urlretrieve(f'{base}/{url_}', f'images/{url_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See category labels for chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "6.804.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
